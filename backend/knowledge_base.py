#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Â∞èËØ¥Áü•ËØÜÂ∫ìÁ≥ªÁªü - ‰ªéÂ∞èËØ¥ÊñáÊú¨‰∏≠ÊèêÂèñÂπ∂Â≠òÂÇ®Áü•ËØÜ‰æõÊ£ÄÁ¥¢‰ΩøÁî®
"""

import json
import re
from typing import List, Dict, Optional, Any
from datetime import datetime
from pathlib import Path
import hashlib
from openai import OpenAI


class NovelKnowledgeBase:
    """Â∞èËØ¥Áü•ËØÜÂ∫ì"""
    
    def __init__(self, storage_dir: str = None, llm_client: OpenAI = None):
        """ÂàùÂßãÂåñÁü•ËØÜÂ∫ì"""
        if storage_dir is None:
            import os
            self.base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            storage_dir = os.path.join(self.base_dir, "novel_kb")
        
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        
        # LLMÂÆ¢Êà∑Á´ØÔºàÁî®‰∫éÊô∫ËÉΩÊèêÂèñÔºâ
        self.llm_client = llm_client
        
        # Áü•ËØÜÂ∫ìÊï∞ÊçÆ
        self.characters: Dict[str, Dict] = {}  # ËßíËâ≤‰ø°ÊÅØ
        self.locations: Dict[str, Dict] = {}   # Âú∞ÁÇπ‰ø°ÊÅØ
        self.events: List[Dict] = []            # ‰∫ã‰ª∂ÂàóË°®
        self.scenes: List[Dict] = []             # Âú∫ÊôØÂàóË°®
        self.summaries: List[Dict] = []         # ÊÆµËêΩÊëòË¶Å
        self.metadata: Dict[str, Any] = {}       # ÂÖÉÊï∞ÊçÆ
        
        self.knowledge_file = self.storage_dir / "knowledge_base.json"
        self.chunks_file = self.storage_dir / "text_chunks.jsonl"
    
    def from_novel_text(self, text: str, novel_name: str = "Êú™ÂëΩÂêçÂ∞èËØ¥") -> bool:
        """‰ªéÂ∞èËØ¥ÊñáÊú¨ÊûÑÂª∫Áü•ËØÜÂ∫ì"""
        try:
            print(f"üìñ ÂºÄÂßã‰ªéÂ∞èËØ¥ÊñáÊú¨ÊûÑÂª∫Áü•ËØÜÂ∫ì: {novel_name}")
            print(f"   ÊñáÊú¨ÈïøÂ∫¶: {len(text)} Â≠óÁ¨¶")
            
            # Ê∏ÖÁ©∫ÊóßÊï∞ÊçÆ
            self.clear()
            print("   ‚úì Â∑≤Ê∏ÖÁ©∫ÊóßÊï∞ÊçÆ")
            
            # ‰øùÂ≠òÂÖÉÊï∞ÊçÆ
            self.metadata = {
                "novel_name": novel_name,
                "created_at": datetime.now().isoformat(),
                "total_length": len(text),
                "total_paragraphs": len([p for p in text.split('\n\n') if p.strip()])
            }
            print("   ‚úì ÂÖÉÊï∞ÊçÆÂ∑≤‰øùÂ≠ò")
            
            # ÊèêÂèñÂêÑ‰∏™Áª¥Â∫¶ÁöÑÁü•ËØÜ
            print("   üìä ÂºÄÂßãÊèêÂèñËßíËâ≤...")
            self._extract_characters(text)
            print(f"   ‚úì ËßíËâ≤ÊèêÂèñÂÆåÊàê: {len(self.characters)} ‰∏™")
            
            print("   üèõÔ∏è  ÂºÄÂßãÊèêÂèñÂú∞ÁÇπ...")
            self._extract_locations(text)
            print(f"   ‚úì Âú∞ÁÇπÊèêÂèñÂÆåÊàê: {len(self.locations)} ‰∏™")
            
            print("   ‚ö° ÂºÄÂßãÊèêÂèñ‰∫ã‰ª∂...")
            self._extract_events(text)
            print(f"   ‚úì ‰∫ã‰ª∂ÊèêÂèñÂÆåÊàê: {len(self.events)} ‰∏™")
            
            print("   üìù ÂºÄÂßãÂàÜÂâ≤ÊñáÊú¨Âùó...")
            self._split_into_chunks(text)
            print(f"   ‚úì ÊñáÊú¨ÂùóÂàÜÂâ≤ÂÆåÊàê: {len(self.chunks) if hasattr(self, 'chunks') else 0} ‰∏™")
            
            print("   üìÑ ÂºÄÂßãÁîüÊàêÊëòË¶Å...")
            self._generate_summaries(text)
            print(f"   ‚úì ÊëòË¶ÅÁîüÊàêÂÆåÊàê: {len(self.summaries)} ÊÆµ")
            
            # ‰øùÂ≠òÁü•ËØÜÂ∫ì
            print("   üíæ ÂºÄÂßã‰øùÂ≠òÁü•ËØÜÂ∫ì...")
            save_success = self.save()
            if not save_success:
                print("   ‚ùå ‰øùÂ≠òÁü•ËØÜÂ∫ìÂ§±Ë¥•!")
                return False
            print("   ‚úì Áü•ËØÜÂ∫ì‰øùÂ≠òÊàêÂäü")
            
            print(f"‚úÖ Áü•ËØÜÂ∫ìÊûÑÂª∫ÂÆåÊàê!")
            print(f"   - ËßíËâ≤: {len(self.characters)} ‰∏™")
            print(f"   - Âú∞ÁÇπ: {len(self.locations)} ‰∏™")
            print(f"   - ‰∫ã‰ª∂: {len(self.events)} ‰∏™")
            print(f"   - Âú∫ÊôØ: {len(self.chunks) if hasattr(self, 'chunks') else 0} ‰∏™")
            print(f"   - ÊÆµËêΩ: {len(self.summaries)} ÊÆµ")
            
            return True
        except Exception as e:
            print(f"‚ùå ÊûÑÂª∫Áü•ËØÜÂ∫ìÂ§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_characters(self, text: str):
        """ÊèêÂèñËßíËâ≤‰ø°ÊÅØÔºà‰ºòÂÖà‰ΩøÁî®AIÔºåÈôçÁ∫ßÂà∞Ê≠£ÂàôÔºâ"""
        # ‰ºòÂÖàÂ∞ùËØïAIÊèêÂèñ
        if self.llm_client:
            try:
                print("ü§ñ ‰ΩøÁî®AIÊô∫ËÉΩÊèêÂèñËßíËâ≤‰ø°ÊÅØ...")
                ai_result = self._extract_characters_with_ai(text)
                if ai_result:
                    # AIÊàêÂäüÊèêÂèñÔºåÂêàÂπ∂Âà∞charactersÂ≠óÂÖ∏
                    for char in ai_result:
                        name = char["name"]
                        first_pos = text.find(name)
                        if first_pos != -1:
                            self.characters[name] = {
                                "name": name,
                                "first_mention": first_pos,
                                "description": char.get("description", ""),
                                "traits": char.get("traits", []),
                                "role": char.get("role", ""),
                                "mentions": text.count(name)
                            }
                    print(f"‚úÖ AIÊèêÂèñÂà∞ {len(ai_result)} ‰∏™ËßíËâ≤")
                    return
            except Exception as e:
                print(f"‚ö†Ô∏è AIÊèêÂèñÂ§±Ë¥•ÔºåÈôçÁ∫ßÂà∞Ê≠£ÂàôË°®ËææÂºè: {e}")
        
        # ÈôçÁ∫ßÊñπÊ°àÔºö‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè
        print("üìù ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊèêÂèñËßíËâ≤‰ø°ÊÅØ...")
        self._extract_characters_with_regex(text)
    
    def _extract_locations(self, text: str):
        """ÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØÔºà‰ºòÂÖà‰ΩøÁî®AIÔºåÈôçÁ∫ßÂà∞Ê≠£ÂàôÔºâ"""
        # ‰ºòÂÖàÂ∞ùËØïAIÊèêÂèñ
        if self.llm_client:
            try:
                print("ü§ñ ‰ΩøÁî®AIÊô∫ËÉΩÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØ...")
                ai_result = self._extract_locations_with_ai(text)
                if ai_result:
                    # AIÊàêÂäüÊèêÂèñ
                    for loc in ai_result:
                        name = loc["name"]
                        first_pos = text.find(name)
                        if first_pos != -1:
                            self.locations[name] = {
                                "name": name,
                                "first_mention": first_pos,
                                "description": loc.get("description", ""),
                                "type": loc.get("type", ""),
                                "mentions": text.count(name)
                            }
                    print(f"‚úÖ AIÊèêÂèñÂà∞ {len(ai_result)} ‰∏™Âú∞ÁÇπ")
                    return
            except Exception as e:
                print(f"‚ö†Ô∏è AIÊèêÂèñÂ§±Ë¥•ÔºåÈôçÁ∫ßÂà∞Ê≠£ÂàôË°®ËææÂºè: {e}")
        
        # ÈôçÁ∫ßÊñπÊ°àÔºö‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè
        print("üìù ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØ...")
        self._extract_locations_with_regex(text)
    
    def _extract_events(self, text: str):
        """ÊèêÂèñ‰∫ã‰ª∂‰ø°ÊÅØ"""
        # ‰∫ã‰ª∂Âä®‰ΩúËØç
        action_markers = ['ÊîªÂáª', 'ÂáªË¥•', 'ÈÅáËßÅ', 'ÊùÄÊ≠ª', 'Êïë‰∏ã', 'ÂèëÁé∞', 'ÊâæÂà∞', 'Â§∫Âõû', 'ÊëßÊØÅ', 'Á†¥Âùè']
        
        for action in action_markers:
            # ÊèêÂèñ‰∏éÂä®‰ΩúÁõ∏ÂÖ≥ÁöÑÂè•Â≠ê
            sentences = re.split(r'[„ÄÇÔºÅÔºü\n]', text)
            for i, sentence in enumerate(sentences):
                if action in sentence:
                    # ÊèêÂèñÁõ∏ÂÖ≥ËßíËâ≤ÂíåÂú∞ÁÇπ
                    chars = [c for c in self.characters.keys() if c in sentence]
                    locs = [l for l in self.locations.keys() if l in sentence]
                    
                    if chars or locs:
                        self.events.append({
                            "type": action,
                            "description": sentence.strip(),
                            "characters": chars,
                            "locations": locs,
                            "position": len(''.join(sentences[:i]))
                        })
    
    def _split_into_chunks(self, text: str, chunk_size: int = 500):
        """Â∞ÜÊñáÊú¨ÂàÜÂâ≤ÊàêÂèØÊ£ÄÁ¥¢ÁöÑÁâáÊÆµ"""
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        chunks = []
        for i in range(0, len(paragraphs), 3):  # ÊØèÊ¨°Âèñ3‰∏™ÊÆµËêΩ
            chunk_paras = paragraphs[i:i+3]
            chunk_text = '\n\n'.join(chunk_paras)
            
            # ËÆ°ÁÆóchunkÁöÑÂêëÈáèË°®Á§∫ÁöÑÁÆÄÂçïÊ®°ÊãüÔºà‰ΩøÁî®ËØçÈ¢ëÔºâ
            words = re.findall(r'[\u4e00-\u9fa5]+', chunk_text)
            vector = list(set(words[:50]))  # ÂèñÂâç50‰∏™‰∏çÂêåÁöÑËØç‰Ωú‰∏∫ÂêëÈáè
            
            chunks.append({
                "id": len(chunks),
                "text": chunk_text,
                "vector": vector,
                "paragraph_indices": [i + j for j in range(len(chunk_paras))],
                "length": len(chunk_text)
            })
        
        self.chunks = chunks
    
    def _generate_summaries(self, text: str):
        """‰∏∫ÊØè‰∏™ÊÆµËêΩÁîüÊàêÊëòË¶Å"""
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for para in paragraphs:
            # ÁÆÄÂçïÁöÑÊëòË¶ÅÔºöÂèñÂâç‰∏§Âè•ËØù
            sentences = re.split(r'[„ÄÇÔºÅÔºü]', para)
            summary = ''.join(sentences[:2]) if len(sentences) > 2 else para
            
            self.summaries.append({
                "original": para,
                "summary": summary[:200],
                "length": len(para)
            })
    
    def retrieve_relevant_content(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """Ê†πÊçÆÊü•ËØ¢Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÂÜÖÂÆπ"""
        # ÁÆÄÂçïÁöÑÂÖ≥ÈîÆËØçÂåπÈÖçÊ£ÄÁ¥¢
        query_words = set(re.findall(r'[\u4e00-\u9fa5]{2,}', query))
        
        results = {
            "characters": [],
            "locations": [],
            "events": [],
            "scenes": [],
            "summaries": []
        }
        
        # Ê£ÄÁ¥¢ËßíËâ≤
        for name, char_info in self.characters.items():
            score = 0
            for word in query_words:
                if word in char_info["name"] or word in char_info["description"]:
                    score += 1
            if score > 0:
                results["characters"].append({**char_info, "relevance_score": score})
        
        # Ê£ÄÁ¥¢Âú∞ÁÇπ
        for name, loc_info in self.locations.items():
            score = sum(1 for word in query_words if word in loc_info["name"])
            if score > 0:
                results["locations"].append({**loc_info, "relevance_score": score})
        
        # Ê£ÄÁ¥¢‰∫ã‰ª∂
        for event in self.events:
            score = sum(1 for word in query_words if word in event["description"])
            if score > 0:
                results["events"].append({**event, "relevance_score": score})
        
        # Ê£ÄÁ¥¢Âú∫ÊôØÁâáÊÆµ
        if hasattr(self, 'chunks'):
            for chunk in self.chunks:
                score = sum(1 for word in query_words if word in chunk["text"])
                if score > 0:
                    results["scenes"].append({
                        "id": chunk["id"],
                        "text": chunk["text"][:300],
                        "relevance_score": score
                    })
        
        # Ê£ÄÁ¥¢ÊëòË¶Å
        for summary in self.summaries:
            score = sum(1 for word in query_words if word in summary["original"])
            if score > 0:
                results["summaries"].append({
                    "original": summary["original"][:300],
                    "summary": summary["summary"],
                    "relevance_score": score
                })
        
        # ÊéíÂ∫èÂπ∂ËøîÂõûÂâçtop_k‰∏™
        for key in results.keys():
            results[key] = sorted(results[key], key=lambda x: x.get("relevance_score", 0), reverse=True)[:top_k]
        
        return results
    
    def get_context_for_generation(self, context_type: str, keywords: List[str] = None) -> str:
        """Ëé∑ÂèñÁî®‰∫éLLMÁîüÊàêÁöÑ‰∏ä‰∏ãÊñá"""
        query = ' '.join(keywords or [])
        relevant = self.retrieve_relevant_content(query, top_k=3)
        
        contexts = []
        
        if context_type in ["character", "all"] and relevant["characters"]:
            contexts.append("„ÄêËßíËâ≤‰ø°ÊÅØ„Äë")
            for char in relevant["characters"]:
                contexts.append(f"- {char['name']}: {char['description']}")
        
        if context_type in ["location", "all"] and relevant["locations"]:
            contexts.append("„ÄêÂú∞ÁÇπ‰ø°ÊÅØ„Äë")
            for loc in relevant["locations"]:
                contexts.append(f"- {loc['name']}: {loc['description']}")
        
        if context_type in ["event", "all"] and relevant["events"]:
            contexts.append("„ÄêÈáçË¶Å‰∫ã‰ª∂„Äë")
            for event in relevant["events"]:
                contexts.append(f"- {event['description']}")
        
        if context_type in ["scene", "all"] and relevant["scenes"]:
            contexts.append("„ÄêÁõ∏ÂÖ≥Âú∫ÊôØ„Äë")
            for scene in relevant["scenes"][:3]:
                contexts.append(f"- {scene['text']}")
        
        return '\n'.join(contexts) if contexts else "ÊöÇÊó†Áõ∏ÂÖ≥ËÉåÊôØ‰ø°ÊÅØ"
    
    def save(self) -> bool:
        """‰øùÂ≠òÁü•ËØÜÂ∫ìÂà∞Êñá‰ª∂"""
        try:
            data = {
                "metadata": self.metadata,
                "characters": self.characters,
                "locations": self.locations,
                "events": self.events,
                "scenes": self.scenes
            }
            
            with open(self.knowledge_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # ‰øùÂ≠òÊñáÊú¨ÁâáÊÆµ
            with open(self.chunks_file, 'w', encoding='utf-8') as f:
                for chunk in getattr(self, 'chunks', []):
                    # ‰∏ç‰øùÂ≠òÂêëÈáèÔºåËäÇÁúÅÁ©∫Èó¥
                    chunk_copy = {k: v for k, v in chunk.items() if k != 'vector'}
                    f.write(json.dumps(chunk_copy, ensure_ascii=False) + '\n')
            
            # ‰øùÂ≠òÊëòË¶Å
            summaries_file = self.storage_dir / "summaries.json"
            with open(summaries_file, 'w', encoding='utf-8') as f:
                json.dump(self.summaries, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠òÁü•ËØÜÂ∫ìÂ§±Ë¥•: {e}")
            return False
    
    def load(self) -> bool:
        """‰ªéÊñá‰ª∂Âä†ËΩΩÁü•ËØÜÂ∫ì"""
        try:
            if not self.knowledge_file.exists():
                return False
            
            with open(self.knowledge_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.metadata = data.get("metadata", {})
                self.characters = data.get("characters", {})
                self.locations = data.get("locations", {})
                self.events = data.get("events", [])
                self.scenes = data.get("scenes", [])
            
            # Âä†ËΩΩÊñáÊú¨ÁâáÊÆµ
            if self.chunks_file.exists():
                self.chunks = []
                with open(self.chunks_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            self.chunks.append(json.loads(line))
            
            # Âä†ËΩΩÊëòË¶Å
            summaries_file = self.storage_dir / "summaries.json"
            if summaries_file.exists():
                with open(summaries_file, 'r', encoding='utf-8') as f:
                    self.summaries = json.load(f)
            
            return True
        except Exception as e:
            print(f"‚ùå Âä†ËΩΩÁü•ËØÜÂ∫ìÂ§±Ë¥•: {e}")
            return False
    
    def clear(self):
        """Ê∏ÖÁ©∫Áü•ËØÜÂ∫ì"""
        self.characters.clear()
        self.locations.clear()
        self.events.clear()
        self.scenes.clear()
        self.summaries.clear()
        self.metadata.clear()
        if hasattr(self, 'chunks'):
            self.chunks.clear()
    
    def get_stats(self) -> Dict:
        """Ëé∑ÂèñÁü•ËØÜÂ∫ìÁªüËÆ°‰ø°ÊÅØ"""
        return {
            "novel_name": self.metadata.get("novel_name", "Êú™ÂëΩÂêç"),
            "characters_count": len(self.characters),
            "locations_count": len(self.locations),
            "events_count": len(self.events),
            "scenes_count": len(getattr(self, 'chunks', [])),
            "summaries_count": len(self.summaries),
            "total_length": self.metadata.get("total_length", 0)
        }
    
    def get_statistics(self) -> Dict:
        """Ëé∑ÂèñÁü•ËØÜÂ∫ìÁªüËÆ°‰ø°ÊÅØÔºàÂÖºÂÆπË∞ÉÁî®Ôºâ"""
        stats = self.get_stats()
        
        # Ê∑ªÂä†ÊéíÂ∫èÂêéÁöÑËßíËâ≤ÂíåÂú∞ÁÇπÂàóË°®
        top_characters = sorted(
            self.characters.keys(),
            key=lambda x: self.characters[x].get('mentions', 0),
            reverse=True
        )
        
        top_locations = sorted(
            self.locations.keys(),
            key=lambda x: self.locations[x].get('mentions', 0),
            reverse=True
        )
        
        return {
            **stats,
            "top_characters": top_characters,
            "top_locations": top_locations
        }
    
    def to_dict(self) -> Dict:
        """Â∞ÜÁü•ËØÜÂ∫ìËΩ¨Êç¢‰∏∫Â≠óÂÖ∏"""
        return {
            "metadata": self.metadata,
            "characters": self.characters,
            "locations": self.locations,
            "events": self.events,
            "scenes": getattr(self, 'chunks', []),
            "summaries": self.summaries
        }
    
    def _extract_characters_with_ai(self, text: str) -> Optional[List[Dict]]:
        """‰ΩøÁî®AIÊèêÂèñËßíËâ≤‰ø°ÊÅØ"""
        if not self.llm_client:
            return None
            
        # Êà™ÂèñÂâç3000Â≠óËøõË°åÂàÜÊûêÔºàÈÅøÂÖçtokenËøáÈïøÔºâ
        sample_text = text[:3000] if len(text) > 3000 else text
        
        prompt = f"""ËØ∑ÂàÜÊûê‰ª•‰∏ãÂ∞èËØ¥ÁâáÊÆµÔºåÊèêÂèñÂÖ∂‰∏≠ÁöÑ‰∏ªË¶ÅËßíËâ≤‰ø°ÊÅØ„ÄÇ

Â∞èËØ¥ÊñáÊú¨ÁâáÊÆµÔºö
---
{sample_text}
---

ËØ∑‰ª•JSONÊ†ºÂºèËøîÂõûËßíËâ≤ÂàóË°®ÔºåÊØè‰∏™ËßíËâ≤ÂåÖÂê´Ôºö
- name: ËßíËâ≤ÂêçÁß∞
- description: ËßíËâ≤ÁöÑÁÆÄË¶ÅÊèèËø∞Ôºà50-100Â≠óÔºâ
- traits: ËßíËâ≤ÁâπÂæÅÔºàÂàóË°®ÔºåÂ¶ÇÔºö["ÂãáÊï¢", "ÂñÑËâØ"]Ôºâ
- role: ËßíËâ≤Á±ªÂûãÔºàÂ¶ÇÔºö‰∏ªËßí„ÄÅÈÖçËßí„ÄÅË∑Ø‰∫∫Á≠âÔºâ

Âè™ÈúÄË¶ÅËøîÂõûJSONÊï∞ÁªÑÔºå‰∏çË¶ÅÊúâÂÖ∂‰ªñËØ¥ÊòéÊñáÂ≠ó„ÄÇÊ†ºÂºèÂøÖÈ°ªÁ¨¶Âêà‰∏•Ê†ºJSONËßÑËåÉ„ÄÇ
Ëá≥Â∞ëÊèêÂèñ3-5‰∏™ÈáçË¶ÅËßíËâ≤„ÄÇ

Á§∫‰æãÊ†ºÂºèÔºö
[{{"name": "ËßíËâ≤Âêç", "description": "ÊèèËø∞...", "traits": ["ÁâπÂæÅ1", "ÁâπÂæÅ2"], "role": "‰∏ªËßí"}}]"""
        
        try:
            print(f"   Ê≠£Âú®Ë∞ÉÁî® SiliconFlow API (Ë∂ÖÊó∂Êó∂Èó¥: 60Áßí)...")
            response = self.llm_client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[
                    {"role": "system", "content": "‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÂ∞èËØ¥ÂàÜÊûêÂä©ÊâãÔºåÊìÖÈïø‰ªéÊñáÊú¨‰∏≠ÊèêÂèñËßíËâ≤‰ø°ÊÅØ„ÄÇ"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
                timeout=60.0  # Ê∑ªÂä†60ÁßíË∂ÖÊó∂
            )
            
            result_text = response.choices[0].message.content.strip()
            print(f"   API ÂìçÂ∫îÈïøÂ∫¶: {len(result_text)} Â≠óÁ¨¶")
            
            # ÊèêÂèñJSONÔºàÂèØËÉΩË¢´markdown‰ª£Á†ÅÂùóÂåÖË£πÔºâ
            json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
            if json_match:
                result_text = json_match.group(0)
            
            characters_data = json.loads(result_text)
            print(f"   ‚úÖ AI ÊèêÂèñÊàêÂäü: {len(characters_data)} ‰∏™ËßíËâ≤")
            return characters_data
            
        except Exception as e:
            print(f"‚ùå AIËßíËâ≤ÊèêÂèñÂ§±Ë¥•: {e}")
            print(f"   ÈîôËØØÁ±ªÂûã: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return None
    
    def _extract_characters_with_regex(self, text: str):
        """‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊèêÂèñËßíËâ≤‰ø°ÊÅØÔºàÈôçÁ∫ßÊñπÊ°àÔºâ"""
        # Â∏∏ËßÅÁöÑËßíËâ≤Âá∫Âú∫Ê®°Âºè
        patterns = [
            r'([\u4e00-\u9fa5]{2,4})(?:ËØ¥|ÈÅì|ÂõûÁ≠î|ÈóÆ|Á¨ëÈÅì|ÂèπÈÅì|ËØ¥ÈÅì)',
            r'([\u4e00-\u9fa5]{2,4})[Ôºå:Ôºö](?:Â•π|‰ªñ|ÂÆÉ)',
        ]
        
        # ÊèêÂèñÂèØËÉΩÁöÑËßíËâ≤Âêç
        potential_chars = set()
        for pattern in patterns:
            try:
                matches = re.finditer(pattern, text)
                for match in matches:
                    name = match.group(1)
                    if len(name) >= 2 and len(name) <= 4:
                        potential_chars.add(name)
            except Exception as e:
                print(f"Ë≠¶Âëä: Ê≠£ÂàôÂåπÈÖçÂ§±Ë¥•: {e}")
        
        # ÂØπËßíËâ≤ÂêçËøõË°åËøáÊª§ÔºàÊéíÈô§Â∏∏ËßÅÈùûËßíËâ≤ËØçÔºâ
        exclude_words = {'ÈÇ£‰∏™', 'Ëøô‰∏™', '‰ªÄ‰πà', 'Â¶Ç‰Ωï', 'Âõ†‰∏∫', 'ÊâÄ‰ª•', 'ËÄå‰∏î', '‰ΩÜÊòØ', 'Â¶ÇÊûú'}
        potential_chars = {c for c in potential_chars if c not in exclude_words}
        
        # Êü•ÊâæÊØè‰∏™ËßíËâ≤ÁöÑÊèèËø∞
        for name in potential_chars:
            # Êü•ÊâæËßíËâ≤È¶ñÊ¨°Âá∫Áé∞ÁöÑ‰ΩçÁΩÆ
            first_pos = text.find(name)
            if first_pos == -1:
                continue
            
            # ÊèêÂèñ‰∏ä‰∏ãÊñáÔºàÂâçÂêé200Â≠óÔºâ
            context_start = max(0, first_pos - 200)
            context_end = min(len(text), first_pos + len(name) + 200)
            context = text[context_start:context_end].replace('\n', ' ')
            
            self.characters[name] = {
                "name": name,
                "first_mention": first_pos,
                "description": context[:150],
                "traits": [],
                "role": "Êú™Áü•",
                "mentions": text.count(name)
            }
    
    def _extract_locations_with_ai(self, text: str) -> Optional[List[Dict]]:
        """‰ΩøÁî®AIÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØ"""
        if not self.llm_client:
            return None
            
        # Êà™ÂèñÂâç3000Â≠óËøõË°åÂàÜÊûê
        sample_text = text[:3000] if len(text) > 3000 else text
        
        prompt = f"""ËØ∑ÂàÜÊûê‰ª•‰∏ãÂ∞èËØ¥ÁâáÊÆµÔºåÊèêÂèñÂÖ∂‰∏≠ÁöÑ‰∏ªË¶ÅÂú∞ÁÇπ‰ø°ÊÅØ„ÄÇ

Â∞èËØ¥ÊñáÊú¨ÁâáÊÆµÔºö
---
{sample_text}
---

ËØ∑‰ª•JSONÊ†ºÂºèËøîÂõûÂú∞ÁÇπÂàóË°®ÔºåÊØè‰∏™Âú∞ÁÇπÂåÖÂê´Ôºö
- name: Âú∞ÁÇπÂêçÁß∞
- description: Âú∞ÁÇπÁöÑÁÆÄË¶ÅÊèèËø∞Ôºà50-100Â≠óÔºâ
- type: Âú∞ÁÇπÁ±ªÂûãÔºàÂ¶ÇÔºöÂüéÂ∏Ç„ÄÅÊùëÂ∫Ñ„ÄÅÂª∫Á≠ë„ÄÅËá™ÁÑ∂ÊôØËßÇÁ≠âÔºâ

Âè™ÈúÄË¶ÅËøîÂõûJSONÊï∞ÁªÑÔºå‰∏çË¶ÅÊúâÂÖ∂‰ªñËØ¥ÊòéÊñáÂ≠ó„ÄÇÊ†ºÂºèÂøÖÈ°ªÁ¨¶Âêà‰∏•Ê†ºJSONËßÑËåÉ„ÄÇ
Ëá≥Â∞ëÊèêÂèñ3-5‰∏™ÈáçË¶ÅÂú∞ÁÇπ„ÄÇ

Á§∫‰æãÊ†ºÂºèÔºö
[{{"name": "Âú∞ÁÇπÂêç", "description": "ÊèèËø∞...", "type": "ÂüéÂ∏Ç"}}]"""
        
        try:
            print(f"   Ê≠£Âú®Ë∞ÉÁî® SiliconFlow API (Ë∂ÖÊó∂Êó∂Èó¥: 60Áßí)...")
            response = self.llm_client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[
                    {"role": "system", "content": "‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÂ∞èËØ¥Âú∫ÊôØÂàÜÊûêÂä©ÊâãÔºåÊìÖÈïø‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØ„ÄÇ"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
                timeout=60.0  # Ê∑ªÂä†60ÁßíË∂ÖÊó∂
            )
            
            result_text = response.choices[0].message.content.strip()
            print(f"   API ÂìçÂ∫îÈïøÂ∫¶: {len(result_text)} Â≠óÁ¨¶")
            
            # ÊèêÂèñJSON
            json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
            if json_match:
                result_text = json_match.group(0)
            
            locations_data = json.loads(result_text)
            print(f"   ‚úÖ AI ÊèêÂèñÊàêÂäü: {len(locations_data)} ‰∏™Âú∞ÁÇπ")
            return locations_data
            
        except Exception as e:
            print(f"‚ùå AIÂú∞ÁÇπÊèêÂèñÂ§±Ë¥•: {e}")
            print(f"   ÈîôËØØÁ±ªÂûã: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return None
    
    def _extract_locations_with_regex(self, text: str):
        """‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊèêÂèñÂú∞ÁÇπ‰ø°ÊÅØÔºàÈôçÁ∫ßÊñπÊ°àÔºâ"""
        # Âú∞ÁÇπÂÖ≥ÈîÆËØç
        location_markers = ['Âú®', 'Êù•Âà∞', 'ËøõÂÖ•', 'Á¶ªÂºÄ', 'ÂâçÂæÄ', 'ËøîÂõû', 'Ë∫´Â§Ñ', 'Á´ôÂú®']
        
        # ÊèêÂèñÂú∞ÁÇπÊ®°Âºè
        for marker in location_markers:
            pattern = f'{marker}([\u4e00-\u9fa5]{2,10}(?:Â∏Ç|Èïá|Êùë|Âüé|Â±±|Ê≤≥|Ë∑Ø|Ë°ó|Êàø|Â±ã|ÈòÅ|Â∫ô|ÊÆø|Ê¥û|Ê£ÆÊûó|ËçâÂéü|Êµ∑Êª©|Ê∏ØÂè£|ËæπÂ¢É|Âπ≥Âéü|Â≥°Ë∞∑)'
            try:
                matches = re.finditer(pattern, text)
                for match in matches:
                    location = match.group(1) + match.group(0)[match.end(1):]
                    if len(location) >= 2 and location not in self.locations:
                        pos = match.start()
                        context_start = max(0, pos - 100)
                        context_end = min(len(text), pos + len(match.group(0)) + 100)
                        self.locations[location] = {
                            "name": location,
                            "first_mention": pos,
                            "description": text[context_start:context_end].replace('\n', ' ')[:100],
                            "type": "Êú™Áü•",
                            "mentions": text.count(location)
                        }
            except Exception as e:
                print(f"Ë≠¶Âëä: Ê≠£ÂàôÂåπÈÖçÂ§±Ë¥•: {e}")